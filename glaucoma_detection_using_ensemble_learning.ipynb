{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGCA2BM1GEj-"
      },
      "outputs": [],
      "source": [
        "!unzip /content/Glaucoma.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:18:23.767710Z",
          "iopub.status.busy": "2025-02-18T12:18:23.767435Z",
          "iopub.status.idle": "2025-02-18T12:18:23.771453Z",
          "shell.execute_reply": "2025-02-18T12:18:23.770576Z",
          "shell.execute_reply.started": "2025-02-18T12:18:23.767690Z"
        },
        "id": "3f6BM0LnepC2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:18:43.418422Z",
          "iopub.status.busy": "2025-02-18T12:18:43.418079Z",
          "iopub.status.idle": "2025-02-18T12:18:43.425066Z",
          "shell.execute_reply": "2025-02-18T12:18:43.424311Z",
          "shell.execute_reply.started": "2025-02-18T12:18:43.418399Z"
        },
        "id": "T55cAmIbepC9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "file_path = \"/content/ORIGA/ORIGA/Images\"\n",
        "\n",
        "# Check if the directory exists before listing files\n",
        "if os.path.exists(file_path) and os.path.isdir(file_path):\n",
        "    jpg_files = [file for file in os.listdir(file_path) if file.lower().endswith(\".jpg\")]\n",
        "    print(\"Number of images:\", len(jpg_files))\n",
        "else:\n",
        "    print(\"Directory not found:\", file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:18:55.616269Z",
          "iopub.status.busy": "2025-02-18T12:18:55.615943Z",
          "iopub.status.idle": "2025-02-18T12:18:55.652508Z",
          "shell.execute_reply": "2025-02-18T12:18:55.651605Z",
          "shell.execute_reply.started": "2025-02-18T12:18:55.616242Z"
        },
        "id": "KMhkTX50epC-",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "file_path = \"/content/glaucoma.csv\"\n",
        "\n",
        "# Check if file exists\n",
        "if os.path.exists(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    print(f\"Number of rows: {data.shape[0]}\")\n",
        "    print(f\"Number of columns: {data.shape[1]}\")\n",
        "    print(\"Column names:\", data.columns.tolist())\n",
        "\n",
        "    # Display first few rows for better understanding\n",
        "    display(data.head())  # Works in Jupyter/Kaggle environments\n",
        "else:\n",
        "    print(f\"File not found: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:19:08.829544Z",
          "iopub.status.busy": "2025-02-18T12:19:08.829261Z",
          "iopub.status.idle": "2025-02-18T12:19:08.845079Z",
          "shell.execute_reply": "2025-02-18T12:19:08.844128Z",
          "shell.execute_reply.started": "2025-02-18T12:19:08.829524Z"
        },
        "id": "pxag5NMJepC-",
        "scrolled": true,
        "trusted": true
      },
      "outputs": [],
      "source": [
        "for column in data.columns:\n",
        "    unique_values = data[column].unique()\n",
        "    num_unique = len(unique_values)\n",
        "\n",
        "    print(f\"Column: '{column}' | Unique Values: {num_unique}\")\n",
        "\n",
        "    # Print only first 10 unique values if too many\n",
        "    if num_unique <= 10:\n",
        "        print(\"Values:\", unique_values, \"\\n\")\n",
        "    else:\n",
        "        print(\"First 10 unique values:\", unique_values[:10], \"...\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:19:12.420399Z",
          "iopub.status.busy": "2025-02-18T12:19:12.420117Z",
          "iopub.status.idle": "2025-02-18T12:19:12.441060Z",
          "shell.execute_reply": "2025-02-18T12:19:12.440414Z",
          "shell.execute_reply.started": "2025-02-18T12:19:12.420379Z"
        },
        "id": "Ki87v1FDepC_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "data.info()\n",
        "print(\"\\nMissing Values Per Column:\\n\")\n",
        "print(data.isnull().sum())  # or data.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:19:19.063850Z",
          "iopub.status.busy": "2025-02-18T12:19:19.063523Z",
          "iopub.status.idle": "2025-02-18T12:19:20.017644Z",
          "shell.execute_reply": "2025-02-18T12:19:20.016940Z",
          "shell.execute_reply.started": "2025-02-18T12:19:19.063823Z"
        },
        "id": "6hKvv7FLepDA",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Check if 'Glaucoma' column exists to avoid errors\n",
        "if 'Glaucoma' in data.columns:\n",
        "    # Count occurrences of each class\n",
        "    glaucoma_counts = data['Glaucoma'].value_counts()\n",
        "\n",
        "    # Print the counts to check for imbalance\n",
        "    print(\"Glaucoma distribution:\\n\", glaucoma_counts)\n",
        "\n",
        "    # Plot bar chart to visualize class distribution\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.barplot(x=glaucoma_counts.index, y=glaucoma_counts.values, palette=\"viridis\")\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.title('Distribution of Glaucoma Classes')\n",
        "    plt.xlabel('Glaucoma (0: No, 1: Yes)')\n",
        "    plt.ylabel('Number of Samples')\n",
        "    plt.xticks(rotation=0)\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Column 'Glaucoma' not found in dataset!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:20:22.162571Z",
          "iopub.status.busy": "2025-02-18T12:20:22.162038Z",
          "iopub.status.idle": "2025-02-18T12:20:26.693139Z",
          "shell.execute_reply": "2025-02-18T12:20:26.692400Z",
          "shell.execute_reply.started": "2025-02-18T12:20:22.162545Z"
        },
        "id": "KqFQ_rtBepDB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import pandas as pd\n",
        "\n",
        "# Paths\n",
        "base_dir = \"/content/ORIGA/ORIGA\"\n",
        "csv_file_path = \"/content/glaucoma.csv\"\n",
        "image_dir = os.path.join(base_dir, \"Images\")\n",
        "output_dir = \"/content/organized_data\"\n",
        "\n",
        "# Load the CSV file safely\n",
        "if os.path.exists(csv_file_path):\n",
        "    data = pd.read_csv(csv_file_path)\n",
        "    print(f\"CSV file loaded successfully! Shape: {data.shape}\")\n",
        "else:\n",
        "    raise FileNotFoundError(f\"CSV file not found at: {csv_file_path}\")\n",
        "\n",
        "# Check if 'Glaucoma' and 'Filename' columns exist\n",
        "required_columns = {\"Glaucoma\", \"Filename\"}\n",
        "if not required_columns.issubset(data.columns):\n",
        "    raise KeyError(f\"Missing required columns! Expected {required_columns}, but found {set(data.columns)}\")\n",
        "\n",
        "# Create output directories for 'yes' and 'no'\n",
        "categories = [\"yes\", \"no\"]\n",
        "subfolders = [\"images\", \"csv\"]\n",
        "\n",
        "for category in categories:\n",
        "    for subfolder in subfolders:\n",
        "        os.makedirs(os.path.join(output_dir, category, subfolder), exist_ok=True)\n",
        "\n",
        "# Separate data based on 'Glaucoma' values\n",
        "yes_data = data[data[\"Glaucoma\"] == 1]\n",
        "no_data = data[data[\"Glaucoma\"] == 0]\n",
        "\n",
        "# Function to organize images and save CSVs\n",
        "def organize_data(subset_data, category):\n",
        "    images_folder = os.path.join(output_dir, category, \"images\")\n",
        "    csv_folder = os.path.join(output_dir, category, \"csv\")\n",
        "\n",
        "    copied_files = 0  # Track successful copies\n",
        "\n",
        "    for _, row in subset_data.iterrows():\n",
        "        filename = row[\"Filename\"]\n",
        "        src_path = os.path.join(image_dir, filename)\n",
        "        dest_path = os.path.join(images_folder, filename)\n",
        "\n",
        "        # Copy image only if it exists\n",
        "        if os.path.exists(src_path):\n",
        "            shutil.copy(src_path, dest_path)\n",
        "            copied_files += 1\n",
        "        else:\n",
        "            print(f\"Warning: File not found - {src_path}\")\n",
        "\n",
        "    # Save corresponding CSV in the 'csv' subfolder\n",
        "    csv_path = os.path.join(csv_folder, \"data.csv\")\n",
        "    subset_data.to_csv(csv_path, index=False)\n",
        "\n",
        "    print(f\"{category.capitalize()} Data: {copied_files} images copied, CSV saved at {csv_path}\")\n",
        "\n",
        "# Organize data for 'yes' and 'no' classes\n",
        "organize_data(yes_data, \"yes\")\n",
        "organize_data(no_data, \"no\")\n",
        "\n",
        "print(\"\\n Data organization complete! Images and CSVs are stored in 'yes' and 'no' folders.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:21:34.362336Z",
          "iopub.status.busy": "2025-02-18T12:21:34.361983Z",
          "iopub.status.idle": "2025-02-18T12:21:34.369059Z",
          "shell.execute_reply": "2025-02-18T12:21:34.368324Z",
          "shell.execute_reply.started": "2025-02-18T12:21:34.362312Z"
        },
        "id": "fJ7jHwkLepDF",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Define paths\n",
        "no_path = \"/content/organized_data/no/images\"\n",
        "yes_path = \"/content/organized_data/yes/images\"\n",
        "\n",
        "# Function to count JPG images in a directory\n",
        "def count_images(directory, label):\n",
        "    if os.path.exists(directory):\n",
        "        files = [file for file in os.listdir(directory) if file.lower().endswith(\".jpg\")]\n",
        "        print(f\"Number of images in {label} directory: {len(files)}\")\n",
        "    else:\n",
        "        print(f\"Warning: Directory not found - {directory}\")\n",
        "\n",
        "# Count images in 'no' and 'yes' directories\n",
        "count_images(no_path, \"no\")\n",
        "count_images(yes_path, \"yes\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:22:56.782290Z",
          "iopub.status.busy": "2025-02-18T12:22:56.781910Z",
          "iopub.status.idle": "2025-02-18T12:30:45.147197Z",
          "shell.execute_reply": "2025-02-18T12:30:45.146359Z",
          "shell.execute_reply.started": "2025-02-18T12:22:56.782262Z"
        },
        "id": "BvAFwYSfepDd",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Paths\n",
        "yes_images_folder = \"/content/organized_data/yes/images\"\n",
        "output_dir = \"/content/organized_data/aug_yes\"\n",
        "aug_images_folder = os.path.join(output_dir, \"images\")\n",
        "aug_csv_folder = os.path.join(output_dir, \"csv\")\n",
        "yes_csv_path = \"/content/organized_data/yes/csv/data.csv\"\n",
        "\n",
        "# Load the CSV file\n",
        "yes_data = pd.read_csv(yes_csv_path)\n",
        "\n",
        "# Create output directories for augmented data\n",
        "os.makedirs(aug_images_folder, exist_ok=True)\n",
        "os.makedirs(aug_csv_folder, exist_ok=True)\n",
        "\n",
        "# Define augmentation transformations\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.1,\n",
        "    zoom_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode=\"nearest\"\n",
        ")\n",
        "\n",
        "# List to store new rows for CSV\n",
        "augmented_rows = []\n",
        "\n",
        "# Augment Images and Create New Data\n",
        "for _, row in yes_data.iterrows():\n",
        "    filename = row[\"Filename\"]\n",
        "    src_path = os.path.join(yes_images_folder, filename)\n",
        "\n",
        "    if os.path.exists(src_path):\n",
        "        # Load the original image\n",
        "        img = Image.open(src_path).convert(\"RGB\")  # Ensure 3-channel image\n",
        "        img_array = np.expand_dims(np.array(img), axis=0)\n",
        "\n",
        "        # Save the original image to the new folder\n",
        "        original_path = os.path.join(aug_images_folder, filename)\n",
        "        img.save(original_path)\n",
        "\n",
        "        # Store the original image in CSV\n",
        "        new_row = row.copy()\n",
        "        new_row[\"Filename\"] = filename\n",
        "        augmented_rows.append(new_row)\n",
        "\n",
        "        # Generate 2 augmented images\n",
        "        i = 1\n",
        "        for batch in datagen.flow(img_array, batch_size=1):\n",
        "            augmented_filename = f\"{os.path.splitext(filename)[0]}_{i}.jpg\"\n",
        "            augmented_image_path = os.path.join(aug_images_folder, augmented_filename)\n",
        "\n",
        "            # Convert and save augmented image\n",
        "            augmented_img = Image.fromarray(batch[0].astype(\"uint8\"))\n",
        "            augmented_img.save(augmented_image_path)\n",
        "\n",
        "            # Store augmented image details in CSV\n",
        "            new_augmented_row = row.copy()\n",
        "            new_augmented_row[\"Filename\"] = augmented_filename\n",
        "            augmented_rows.append(new_augmented_row)\n",
        "\n",
        "            i += 1\n",
        "            if i > 2:  # Stop after generating exactly 2 augmented images\n",
        "                break\n",
        "\n",
        "# Save updated CSV\n",
        "augmented_data = pd.DataFrame(augmented_rows)\n",
        "aug_csv_path = os.path.join(aug_csv_folder, \"data.csv\")\n",
        "augmented_data.to_csv(aug_csv_path, index=False)\n",
        "\n",
        "print(f\"Augmented images saved to: {aug_images_folder}\")\n",
        "print(f\"Updated CSV saved to: {aug_csv_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:32:12.878398Z",
          "iopub.status.busy": "2025-02-18T12:32:12.878037Z",
          "iopub.status.idle": "2025-02-18T12:32:12.884286Z",
          "shell.execute_reply": "2025-02-18T12:32:12.883590Z",
          "shell.execute_reply.started": "2025-02-18T12:32:12.878374Z"
        },
        "id": "FnQXGZf5epDf",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Path to the augmented images folder\n",
        "file_path = \"/content/organized_data/aug_yes/images\"\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(file_path) and os.path.isdir(file_path):\n",
        "    # List all files in the directory\n",
        "    all_files = os.listdir(file_path)\n",
        "\n",
        "    # Filter only `.jpg` images\n",
        "    aug_files = [file for file in all_files if file.lower().endswith(\".jpg\")]\n",
        "\n",
        "    # Count the number of images\n",
        "    print(f\"Number of total augmented images: {len(aug_files)}\")\n",
        "else:\n",
        "    print(f\"Error: The directory '{file_path}' does not exist!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:32:32.615950Z",
          "iopub.status.busy": "2025-02-18T12:32:32.615511Z",
          "iopub.status.idle": "2025-02-18T12:32:32.625498Z",
          "shell.execute_reply": "2025-02-18T12:32:32.624682Z",
          "shell.execute_reply.started": "2025-02-18T12:32:32.615915Z"
        },
        "id": "YXV0J5lvepDg",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "# Path to the augmented CSV file\n",
        "file_path = \"/content/organized_data/aug_yes/csv/data.csv\"\n",
        "\n",
        "# Check if the file exists before reading\n",
        "if os.path.exists(file_path) and os.path.isfile(file_path):\n",
        "    try:\n",
        "        # Load the CSV file\n",
        "        df = pd.read_csv(file_path)\n",
        "\n",
        "        # Check if the file is empty\n",
        "        if df.empty:\n",
        "            print(f\"Warning: The CSV file '{file_path}' is empty.\")\n",
        "        else:\n",
        "            # Display CSV details\n",
        "            print(f\"Successfully loaded '{file_path}'\")\n",
        "            print(\"Number of rows:\", df.shape[0])\n",
        "            print(\"Number of columns:\", df.shape[1])\n",
        "            print(\"Column names:\", df.columns.tolist())\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\" Error reading the CSV file: {e}\")\n",
        "\n",
        "else:\n",
        "    print(f\" Error: The file '{file_path}' does not exist!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:33:21.542509Z",
          "iopub.status.busy": "2025-02-18T12:33:21.542194Z",
          "iopub.status.idle": "2025-02-18T12:33:21.679885Z",
          "shell.execute_reply": "2025-02-18T12:33:21.679022Z",
          "shell.execute_reply.started": "2025-02-18T12:33:21.542488Z"
        },
        "id": "IvrhgSeeepDh",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Paths to the image folders\n",
        "aug_yes_images_folder = \"/content/organized_data/aug_yes/images\"\n",
        "no_images_folder = \"/content/organized_data/no/images\"\n",
        "\n",
        "# Function to count images in a folder\n",
        "def count_images(folder_path):\n",
        "    if os.path.exists(folder_path) and os.path.isdir(folder_path):\n",
        "        return len([img for img in os.listdir(folder_path) if img.lower().endswith(\".jpg\")])\n",
        "    else:\n",
        "        print(f\"Warning: The folder '{folder_path}' does not exist!\")\n",
        "        return 0\n",
        "\n",
        "# Count images\n",
        "num_aug_yes_images = count_images(aug_yes_images_folder)\n",
        "num_no_images = count_images(no_images_folder)\n",
        "\n",
        "# Data for the bar chart\n",
        "categories = [\"Augmented Yes\", \"No\"]\n",
        "counts = [num_aug_yes_images, num_no_images]\n",
        "\n",
        "# Check if there's data to plot\n",
        "if sum(counts) == 0:\n",
        "    print(\"No images found in both folders. Skipping the plot.\")\n",
        "else:\n",
        "    # Plot the bar chart\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.bar(categories, counts, color=[\"blue\", \"green\"])\n",
        "    plt.title(\"Comparison of Image Counts Between Augmented Yes and No Folders\")\n",
        "    plt.xlabel(\"Category\")\n",
        "    plt.ylabel(\"Number of Images\")\n",
        "    plt.ylim(0, max(counts) + 10)  # Adjust y-axis for better visualization\n",
        "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:33:25.530412Z",
          "iopub.status.busy": "2025-02-18T12:33:25.530068Z",
          "iopub.status.idle": "2025-02-18T12:33:25.545796Z",
          "shell.execute_reply": "2025-02-18T12:33:25.544862Z",
          "shell.execute_reply.started": "2025-02-18T12:33:25.530388Z"
        },
        "id": "QpyRgZGwepDi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:34:40.461841Z",
          "iopub.status.busy": "2025-02-18T12:34:40.461508Z",
          "iopub.status.idle": "2025-02-18T12:35:05.165973Z",
          "shell.execute_reply": "2025-02-18T12:35:05.165189Z",
          "shell.execute_reply.started": "2025-02-18T12:34:40.461817Z"
        },
        "id": "EbIhRa0DepDi",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Paths for aug_yes and no\n",
        "aug_yes_csv_path = \"/content/organized_data/aug_yes/csv/data.csv\"\n",
        "aug_yes_images_folder = \"/content/organized_data/aug_yes/images\"\n",
        "\n",
        "no_csv_path = \"/content/organized_data/no/csv/data.csv\"\n",
        "no_images_folder = \"/content/organized_data/no/images\"\n",
        "\n",
        "# Load CSVs\n",
        "data_yes = pd.read_csv(aug_yes_csv_path)\n",
        "data_yes['label'] = 1  # Assign label 1 for \"yes\"\n",
        "\n",
        "data_no = pd.read_csv(no_csv_path)\n",
        "data_no['label'] = 0  # Assign label 0 for \"no\"\n",
        "\n",
        "# Combine both classes and shuffle\n",
        "data = pd.concat([data_yes, data_no], ignore_index=True)\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split data into train, validation, and test sets (70:15:15) while maintaining label balance\n",
        "train_data, temp_data = train_test_split(data, test_size=0.3, stratify=data['label'], random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
        "\n",
        "# Function to preprocess images and create data lists\n",
        "def preprocess_and_load_data(data_subset, folder_paths):\n",
        "    images = []\n",
        "    labels = []\n",
        "\n",
        "    for _, row in data_subset.iterrows():\n",
        "        filename = row[\"Filename\"]\n",
        "        label = row[\"label\"]\n",
        "        # Determine the folder based on the label\n",
        "        image_folder = folder_paths['yes'] if label == 1 else folder_paths['no']\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        if os.path.exists(image_path):\n",
        "            # Load the image and resize to (224, 224) for ResNet50\n",
        "            img = load_img(image_path, target_size=(224, 224))\n",
        "            img_array = img_to_array(img)\n",
        "            img_preprocessed = preprocess_input(img_array)  # Preprocess for ResNet50\n",
        "\n",
        "            images.append(img_preprocessed)\n",
        "            labels.append(label)\n",
        "\n",
        "    return np.array(images), np.array(labels)\n",
        "\n",
        "# Define image folder paths for both labels\n",
        "folder_paths = {\n",
        "    \"yes\": aug_yes_images_folder,\n",
        "    \"no\": no_images_folder\n",
        "}\n",
        "\n",
        "# Preprocess and load data for train, validation, and test sets\n",
        "train_images, train_labels = preprocess_and_load_data(train_data, folder_paths)\n",
        "valid_images, valid_labels = preprocess_and_load_data(valid_data, folder_paths)\n",
        "test_images, test_labels = preprocess_and_load_data(test_data, folder_paths)\n",
        "\n",
        "# One-hot encode the labels\n",
        "num_classes = 2  # Glaucoma has two classes: 0 and 1\n",
        "train_labels = to_categorical(train_labels, num_classes=num_classes)\n",
        "valid_labels = to_categorical(valid_labels, num_classes=num_classes)\n",
        "test_labels = to_categorical(test_labels, num_classes=num_classes)\n",
        "\n",
        "# Output the shapes of the data\n",
        "print(f\"Train images shape: {train_images.shape}\")\n",
        "print(f\"Train labels shape: {train_labels.shape}\")\n",
        "print(f\"Validation images shape: {valid_images.shape}\")\n",
        "print(f\"Validation labels shape: {valid_labels.shape}\")\n",
        "print(f\"Test images shape: {test_images.shape}\")\n",
        "print(f\"Test labels shape: {test_labels.shape}\")\n",
        "\n",
        "# Print class distribution in each split\n",
        "print(\"Class distribution in Train:\", np.sum(train_labels, axis=0))\n",
        "print(\"Class distribution in Validation:\", np.sum(valid_labels, axis=0))\n",
        "print(\"Class distribution in Test:\", np.sum(test_labels, axis=0))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:35:13.639704Z",
          "iopub.status.busy": "2025-02-18T12:35:13.639399Z",
          "iopub.status.idle": "2025-02-18T12:35:13.645178Z",
          "shell.execute_reply": "2025-02-18T12:35:13.644260Z",
          "shell.execute_reply.started": "2025-02-18T12:35:13.639681Z"
        },
        "id": "OWtfnOnfepDk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Define data augmentation for the training set\n",
        "data_gen = ImageDataGenerator(\n",
        "    rotation_range=15,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Define ResNet50 model\n",
        "def create_resnet50_model(input_shape, num_classes):\n",
        "    # Load ResNet50 base model with pre-trained weights\n",
        "    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Freeze the base model layers for transfer learning\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom classification layers\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # Compile the model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss=tf.losses.CategoricalCrossentropy(),\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:35:19.883513Z",
          "iopub.status.busy": "2025-02-18T12:35:19.883225Z",
          "iopub.status.idle": "2025-02-18T12:35:19.887810Z",
          "shell.execute_reply": "2025-02-18T12:35:19.887035Z",
          "shell.execute_reply.started": "2025-02-18T12:35:19.883493Z"
        },
        "id": "iW1DCRx0epDk",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "def lr_schedule(epoch, lr):\n",
        "    # Example: Decrease learning rate by 10% every 5 epochs\n",
        "    if epoch % 5 == 0 and epoch > 0:\n",
        "        lr *= 0.9\n",
        "    return float(lr)  # Ensure the return value is a float\n",
        "\n",
        "# Create LearningRateScheduler callback with the corrected schedule function\n",
        "learning_rate_scheduler = LearningRateScheduler(lr_schedule)\n",
        "\n",
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:35:23.626139Z",
          "iopub.status.busy": "2025-02-18T12:35:23.625789Z",
          "iopub.status.idle": "2025-02-18T12:36:55.800262Z",
          "shell.execute_reply": "2025-02-18T12:36:55.799306Z",
          "shell.execute_reply.started": "2025-02-18T12:35:23.626077Z"
        },
        "id": "YJpSNHXAepDl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 2  # Yes and No classes\n",
        "model = create_resnet50_model(input_shape, num_classes)\n",
        "\n",
        "# Training the model\n",
        "history = model.fit(\n",
        "    data_gen.flow(train_images, train_labels, batch_size=32),\n",
        "    epochs=50,\n",
        "    validation_data=(valid_images, valid_labels),\n",
        "    callbacks=[early_stopping, learning_rate_scheduler]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:38:01.382805Z",
          "iopub.status.busy": "2025-02-18T12:38:01.382488Z",
          "iopub.status.idle": "2025-02-18T12:38:02.366344Z",
          "shell.execute_reply": "2025-02-18T12:38:02.365221Z",
          "shell.execute_reply.started": "2025-02-18T12:38:01.382782Z"
        },
        "id": "ZP0INFu2epDl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Create directory if it doesn't exist\n",
        "save_dir = '/content/drive/MyDrive'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "# Ensure file paths are valid\n",
        "weights_path = './resnet50_model_weights.weights.h5'\n",
        "model_path = './resnet50_model.h5'\n",
        "\n",
        "# Debug paths\n",
        "print(f\"Saving weights to: {weights_path}\")\n",
        "print(f\"Saving model to: {model_path}\")\n",
        "\n",
        "# Save weights and model\n",
        "model.save_weights(weights_path)\n",
        "model.save(model_path)\n",
        "\n",
        "print(\"Model and weights saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:38:36.786877Z",
          "iopub.status.busy": "2025-02-18T12:38:36.786535Z",
          "iopub.status.idle": "2025-02-18T12:38:37.447713Z",
          "shell.execute_reply": "2025-02-18T12:38:37.447012Z",
          "shell.execute_reply.started": "2025-02-18T12:38:36.786851Z"
        },
        "id": "nsMw_DucepDl",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Evaluate the model\n",
        "results = model.evaluate(valid_images, valid_labels, batch_size=32)\n",
        "print(f\"Validation Loss: {results[0]}, Validation Accuracy: {results[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:39:06.771756Z",
          "iopub.status.busy": "2025-02-18T12:39:06.771435Z",
          "iopub.status.idle": "2025-02-18T12:39:13.874336Z",
          "shell.execute_reply": "2025-02-18T12:39:13.873628Z",
          "shell.execute_reply.started": "2025-02-18T12:39:06.771731Z"
        },
        "id": "nhHAvDAmepDm",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Debug: Check test shapes and content\n",
        "print(f\"Test images shape: {test_images.shape}\")\n",
        "print(f\"Test labels shape: {test_labels.shape}\")\n",
        "print(f\"Test labels content: {test_labels[:10]}\")  # Sample labels\n",
        "\n",
        "# If test_labels is already in integer format, use it directly\n",
        "test_labels_int = test_labels if len(test_labels.shape) == 1 else np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Load your trained model\n",
        "model = tf.keras.models.load_model(\"/content/resnet50_model.h5\")  # Path to your trained model\n",
        "\n",
        "# Predict using the trained model\n",
        "predictions = model.predict(test_images)\n",
        "predicted_classes = np.argmax(predictions, axis=1)  # Convert predictions to class indices\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(test_labels_int, predicted_classes)\n",
        "print(f\"Manual Test Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "# Classification Report\n",
        "unique_classes = np.unique(test_labels_int)  # Get unique classes\n",
        "target_names = [f\"Class {cls}\" for cls in unique_classes]  # Dynamically create target names\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(test_labels_int, predicted_classes, target_names=target_names))\n",
        "\n",
        "# Confusion Matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "conf_matrix = confusion_matrix(test_labels_int, predicted_classes)\n",
        "print(conf_matrix)\n",
        "\n",
        "# Optional: Visualize the confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title(\"Confusion Matrix (RESNET50)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "#  print predictions for each image\n",
        "for i, (true_label, pred_label) in enumerate(zip(test_labels_int, predicted_classes)):\n",
        "    print(f\"Image {i + 1}: True Label: {true_label}, Predicted Label: {pred_label}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5w3VHSb6LL2E"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(test_labels_int, predictions[:, 1]) # Assuming Glaucoma is class 1. Adjust if different.\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (ResNet50)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9cNLWXjDIPv"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Plot training & validation accuracy values\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy (ResNet50)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss (ResNet50)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:39:28.975900Z",
          "iopub.status.busy": "2025-02-18T12:39:28.975586Z",
          "iopub.status.idle": "2025-02-18T12:39:28.981645Z",
          "shell.execute_reply": "2025-02-18T12:39:28.980871Z",
          "shell.execute_reply.started": "2025-02-18T12:39:28.975874Z"
        },
        "id": "xfyis1TpepDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications import VGG16\n",
        "from tensorflow.keras.layers import Dense, Flatten, Dropout, GlobalAveragePooling2D\n",
        "\n",
        "# Define VGG16 model\n",
        "def create_vgg16_model(input_shape, num_classes):\n",
        "    # Load the VGG16 base model with pre-trained weights\n",
        "    base_model = VGG16(weights='imagenet', include_top=False, input_shape=input_shape)\n",
        "\n",
        "    # Freeze the base model layers for transfer learning\n",
        "    for layer in base_model.layers:\n",
        "        layer.trainable = False\n",
        "\n",
        "    # Add custom classification layers\n",
        "    x = base_model.output\n",
        "    x = Flatten()(x)  # Flatten the output of the base model\n",
        "    x = Dense(256, activation='relu')(x)  # Fully connected layer\n",
        "    x = Dropout(0.5)(x)  # Dropout for regularization\n",
        "    predictions = Dense(num_classes, activation='softmax')(x)  # Final classification layer\n",
        "\n",
        "    # Compile the model\n",
        "    model = Model(inputs=base_model.input, outputs=predictions)\n",
        "    model.compile(optimizer=Adam(learning_rate=1e-4),\n",
        "                  loss='categorical_crossentropy',\n",
        "                  metrics=['accuracy'])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:39:34.313678Z",
          "iopub.status.busy": "2025-02-18T12:39:34.313392Z",
          "iopub.status.idle": "2025-02-18T12:39:34.317787Z",
          "shell.execute_reply": "2025-02-18T12:39:34.316758Z",
          "shell.execute_reply.started": "2025-02-18T12:39:34.313656Z"
        },
        "id": "NgvFccUEepDn",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
        "learning_rate_scheduler = LearningRateScheduler(lr_schedule)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:39:39.134072Z",
          "iopub.status.busy": "2025-02-18T12:39:39.133765Z",
          "iopub.status.idle": "2025-02-18T12:41:39.583603Z",
          "shell.execute_reply": "2025-02-18T12:41:39.582407Z",
          "shell.execute_reply.started": "2025-02-18T12:39:39.134049Z"
        },
        "id": "iSSrJqrWepDo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Model configuration\n",
        "input_shape = (224, 224, 3)\n",
        "num_classes = 2  # Yes and No classes\n",
        "vgg16_model = create_vgg16_model(input_shape, num_classes)\n",
        "\n",
        "# Train the VGG16 model\n",
        "history_vgg16 = vgg16_model.fit(\n",
        "    train_images,  # Training images\n",
        "    train_labels,  # One-hot encoded training labels\n",
        "    batch_size=32,\n",
        "    epochs=25,\n",
        "    validation_data=(valid_images, valid_labels),  # Validation data\n",
        "    callbacks=[early_stopping, learning_rate_scheduler]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:41:43.678817Z",
          "iopub.status.busy": "2025-02-18T12:41:43.678470Z",
          "iopub.status.idle": "2025-02-18T12:41:44.115557Z",
          "shell.execute_reply": "2025-02-18T12:41:44.114663Z",
          "shell.execute_reply.started": "2025-02-18T12:41:43.678788Z"
        },
        "id": "FBp2s3OpepDo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Save the VGG16 model and weights\n",
        "vgg16_weights_path = '/content/vgg16_model_weights.weights.h5'\n",
        "vgg16_model_path = '/content/vgg16_model.h5'\n",
        "\n",
        "vgg16_model.save_weights(vgg16_weights_path)\n",
        "vgg16_model.save(vgg16_model_path)\n",
        "\n",
        "print(\"VGG16 model and weights saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:42:03.035486Z",
          "iopub.status.busy": "2025-02-18T12:42:03.035126Z",
          "iopub.status.idle": "2025-02-18T12:42:05.147514Z",
          "shell.execute_reply": "2025-02-18T12:42:05.146778Z",
          "shell.execute_reply.started": "2025-02-18T12:42:03.035458Z"
        },
        "id": "nUzn_7_RepDo",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "# Load the trained VGG16 model\n",
        "vgg16_model = load_model('/content/vgg16_model.h5')  # Path to the saved VGG16 model\n",
        "\n",
        "# Debug: Check test data shape and content\n",
        "print(f\"Test images shape: {test_images.shape}\")\n",
        "print(f\"Test labels shape: {test_labels.shape}\")\n",
        "print(f\"Sample test labels (one-hot): {test_labels[:10]}\")\n",
        "\n",
        "# Convert one-hot encoded test labels to integer format\n",
        "test_labels_int = test_labels if len(test_labels.shape) == 1 else np.argmax(test_labels, axis=1)\n",
        "\n",
        "# Predict using the VGG16 model\n",
        "vgg16_predictions = vgg16_model.predict(test_images)\n",
        "vgg16_predicted_classes = np.argmax(vgg16_predictions, axis=1)  # Convert predictions to class indices\n",
        "\n",
        "# Calculate accuracy\n",
        "vgg16_test_accuracy = accuracy_score(test_labels_int, vgg16_predicted_classes)\n",
        "print(f\"VGG16 Test Accuracy: {vgg16_test_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Generate classification report\n",
        "print(\"\\nClassification Report (VGG16):\")\n",
        "target_names = [f\"Class {cls}\" for cls in np.unique(test_labels_int)]  # Dynamically create target names\n",
        "print(classification_report(test_labels_int, vgg16_predicted_classes, target_names=target_names))\n",
        "\n",
        "# Generate confusion matrix\n",
        "print(\"\\nConfusion Matrix (VGG16):\")\n",
        "vgg16_conf_matrix = confusion_matrix(test_labels_int, vgg16_predicted_classes)\n",
        "print(vgg16_conf_matrix)\n",
        "\n",
        "# Optional: Visualize the confusion matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(vgg16_conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=target_names, yticklabels=target_names)\n",
        "plt.title(\"Confusion Matrix (VGG16)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n",
        "\n",
        "# Optionally: Print individual predictions for each test image\n",
        "for i, (true_label, pred_label) in enumerate(zip(test_labels_int, vgg16_predicted_classes)):\n",
        "    print(f\"Image {i + 1}: True Label: {true_label}, Predicted Label: {pred_label}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J6hheUbfMRsQ"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming 'vgg16_predictions' contains probabilities for the positive class (Glaucoma)\n",
        "# and 'test_labels_int' are the true binary labels (0 or 1).\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(test_labels_int, vgg16_predictions[:, 1]) # Assuming Glaucoma is class 1. Adjust if different.\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (VGG16)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c3JJK67sEghJ"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Flatten, Dropout\n",
        "\n",
        "# Plot training & validation accuracy values for VGG16\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history_vgg16.history['accuracy'])\n",
        "plt.plot(history_vgg16.history['val_accuracy'])\n",
        "plt.title('Model accuracy (VGG16)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values for VGG16\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history_vgg16.history['loss'])\n",
        "plt.plot(history_vgg16.history['val_loss'])\n",
        "plt.title('Model loss (VGG16)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T12:43:15.384023Z",
          "iopub.status.busy": "2025-02-18T12:43:15.383658Z",
          "iopub.status.idle": "2025-02-18T13:06:53.547414Z",
          "shell.execute_reply": "2025-02-18T13:06:53.546529Z",
          "shell.execute_reply.started": "2025-02-18T12:43:15.383999Z"
        },
        "id": "y9EjyrzZqgAx",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from PIL import Image\n",
        "import timm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Paths for dataset\n",
        "aug_yes_csv_path = \"/content/organized_data/aug_yes/csv/data.csv\"\n",
        "aug_yes_images_folder = \"/content/organized_data/aug_yes/images\"\n",
        "\n",
        "no_csv_path = \"/content/organized_data/no/csv/data.csv\"\n",
        "no_images_folder = \"/content/organized_data/no/images\"\n",
        "\n",
        "# Load CSVs\n",
        "data_yes = pd.read_csv(aug_yes_csv_path)\n",
        "data_yes['label'] = 1  # Glaucoma detected\n",
        "\n",
        "data_no = pd.read_csv(no_csv_path)\n",
        "data_no['label'] = 0  # No Glaucoma\n",
        "\n",
        "# Combine datasets and shuffle\n",
        "data = pd.concat([data_yes, data_no], ignore_index=True)\n",
        "data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Split into Train (70%), Validation (15%), Test (15%)\n",
        "train_data, temp_data = train_test_split(data, test_size=0.3, stratify=data['label'], random_state=42)\n",
        "valid_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n",
        "\n",
        "# Define image transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(15),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Custom dataset class\n",
        "class GlaucomaDataset(Dataset):\n",
        "    def __init__(self, data, folder_paths, transform=None):\n",
        "        self.data = data\n",
        "        self.folder_paths = folder_paths\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        filename, label = row[\"Filename\"], row[\"label\"]\n",
        "        image_folder = self.folder_paths['yes'] if label == 1 else self.folder_paths['no']\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "        image = Image.open(image_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label\n",
        "\n",
        "# Define folder paths\n",
        "folder_paths = {\"yes\": aug_yes_images_folder, \"no\": no_images_folder}\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = GlaucomaDataset(train_data, folder_paths, transform=transform)\n",
        "valid_dataset = GlaucomaDataset(valid_data, folder_paths, transform=transform)\n",
        "test_dataset = GlaucomaDataset(test_data, folder_paths, transform=transform)\n",
        "\n",
        "# DataLoaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Define Swin Transformer model\n",
        "class SwinGlaucomaClassifier(nn.Module):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super(SwinGlaucomaClassifier, self).__init__()\n",
        "        self.swin = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=True, num_classes=num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.swin(x)\n",
        "\n",
        "# Initialize model\n",
        "model = SwinGlaucomaClassifier(num_classes=2).to(device)\n",
        "\n",
        "# Loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.9)\n",
        "\n",
        "# Training function\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    best_acc = 0.0\n",
        "    train_losses, val_losses = [], []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n",
        "        model.train()\n",
        "        running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels.long())\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, predicted = outputs.max(1)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "        train_loss = running_loss / total\n",
        "        train_acc = correct / total\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss, correct, total = 0.0, 0, 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in valid_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels.long())\n",
        "\n",
        "                val_loss += loss.item() * images.size(0)\n",
        "                _, predicted = outputs.max(1)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "                total += labels.size(0)\n",
        "\n",
        "        val_loss /= total\n",
        "        val_acc = correct / total\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        print(f\"Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
        "\n",
        "        if val_acc > best_acc:\n",
        "            best_acc = val_acc\n",
        "            torch.save(model.state_dict(), \"best_swin_model.pth\")\n",
        "            print(\"Best model saved!\")\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Train the model\n",
        "train_losses, val_losses = train_model(model, train_loader, valid_loader, criterion, optimizer, scheduler, num_epochs=25)\n",
        "\n",
        "# Load best model\n",
        "model.load_state_dict(torch.load(\"best_swin_model.pth\"))\n",
        "\n",
        "# Evaluation\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    print(f\"\\nTest Accuracy: {accuracy * 100:.2f}%\")\n",
        "    print(\"\\nClassification Report:\\n\", classification_report(y_true, y_pred))\n",
        "\n",
        "    # Confusion matrix\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
        "    plt.title(\"Confusion Matrix (Swin Transformer)\")\n",
        "    plt.xlabel(\"Predicted Label\")\n",
        "    plt.ylabel(\"True Label\")\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate the model\n",
        "evaluate_model(model, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uc21MO8tLTEp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def evaluate_model_with_roc(model, test_loader, device):\n",
        "    model.eval()\n",
        "    y_true, y_scores = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            probabilities = torch.softmax(outputs, dim=1)  # Get probabilities for each class\n",
        "            y_scores.extend(probabilities[:, 1].cpu().numpy()) # Probability of the positive class (class 1, assuming binary classification)\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
        "    roc_auc = auc(fpr, tpr)\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % roc_auc)\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curve (Swin Transformer)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate and plot ROC curve\n",
        "evaluate_model_with_roc(model, test_loader, device)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qrNPJjxLnB8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])  # Access accuracy using history.history\n",
        "plt.plot(history.history['val_accuracy'])  # Access validation accuracy using history.history\n",
        "plt.title('Model accuracy (Swin Transformer)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(train_losses)  # Replace with your actual training loss history\n",
        "plt.plot(val_losses)  # Replace with your actual validation loss history\n",
        "plt.title('Model loss (Swin Transformer)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T13:06:53.554503Z",
          "iopub.status.busy": "2025-02-18T13:06:53.554271Z",
          "iopub.status.idle": "2025-02-18T13:06:53.567638Z",
          "shell.execute_reply": "2025-02-18T13:06:53.566846Z",
          "shell.execute_reply.started": "2025-02-18T13:06:53.554484Z"
        },
        "id": "YHqnIUTzxT1j",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define number of classes\n",
        "num_classes = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T13:07:09.323719Z",
          "iopub.status.busy": "2025-02-18T13:07:09.323387Z",
          "iopub.status.idle": "2025-02-18T13:07:13.799397Z",
          "shell.execute_reply": "2025-02-18T13:07:13.798365Z",
          "shell.execute_reply.started": "2025-02-18T13:07:09.323689Z"
        },
        "id": "0QOPicENzr54",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install swin-transformer-pytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T13:07:22.501480Z",
          "iopub.status.busy": "2025-02-18T13:07:22.501141Z",
          "iopub.status.idle": "2025-02-18T13:07:24.258949Z",
          "shell.execute_reply": "2025-02-18T13:07:24.258165Z",
          "shell.execute_reply.started": "2025-02-18T13:07:22.501454Z"
        },
        "id": "IRdOXYPgxxyO",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import timm  # Import timm for SwinTransformer\n",
        "\n",
        "# Load TensorFlow Models\n",
        "resnet_model = load_model(\"/content/resnet50_model.h5\")\n",
        "vgg_model = load_model(\"/content/vgg16_model.h5\")\n",
        "\n",
        "# Define model architecture (Must match the architecture you trained with)\n",
        "# Instantiate SwinTransformer using timm.create_model\n",
        "swin_model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=False, num_classes=2)\n",
        "\n",
        "# Load the state_dict, handling the key prefix\n",
        "state_dict = torch.load(\"best_swin_model.pth\", map_location=torch.device(\"cpu\"))\n",
        "# Remove the 'swin.' prefix from the keys\n",
        "from collections import OrderedDict\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in state_dict.items():\n",
        "    name = k[5:] # remove `swin.`\n",
        "    new_state_dict[name] = v\n",
        "# load params\n",
        "swin_model.load_state_dict(new_state_dict)\n",
        "\n",
        "\n",
        "# Set to evaluation mode\n",
        "swin_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T13:07:54.248947Z",
          "iopub.status.busy": "2025-02-18T13:07:54.248625Z",
          "iopub.status.idle": "2025-02-18T13:07:54.295780Z",
          "shell.execute_reply": "2025-02-18T13:07:54.294986Z",
          "shell.execute_reply.started": "2025-02-18T13:07:54.248925Z"
        },
        "id": "4Lyldkl41AsR",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# Assuming test_images are already preprocessed\n",
        "test_images_tf = test_images  # Used for TensorFlow models\n",
        "test_images_torch = torch.tensor(test_images.transpose(0, 3, 1, 2), dtype=torch.float32)  # For PyTorch Swin Transformer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T13:07:57.822723Z",
          "iopub.status.busy": "2025-02-18T13:07:57.822389Z",
          "iopub.status.idle": "2025-02-18T13:08:27.157460Z",
          "shell.execute_reply": "2025-02-18T13:08:27.156732Z",
          "shell.execute_reply.started": "2025-02-18T13:07:57.822686Z"
        },
        "id": "YCLhhn8L1FJ7",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "# TensorFlow Models (ResNet & VGG)\n",
        "resnet_preds = resnet_model.predict(test_images_tf)\n",
        "vgg_preds = vgg_model.predict(test_images_tf)\n",
        "\n",
        "# Swin Transformer (PyTorch)\n",
        "with torch.no_grad():\n",
        "    swin_preds = swin_model(test_images_torch)\n",
        "    swin_preds = torch.nn.functional.softmax(swin_preds, dim=1).numpy()\n",
        "\n",
        "# Convert predictions to probabilities\n",
        "resnet_probs = np.array(resnet_preds)\n",
        "vgg_probs = np.array(vgg_preds)\n",
        "swin_probs = np.array(swin_preds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T13:08:28.839110Z",
          "iopub.status.busy": "2025-02-18T13:08:28.838775Z",
          "iopub.status.idle": "2025-02-18T13:08:28.843835Z",
          "shell.execute_reply": "2025-02-18T13:08:28.842720Z",
          "shell.execute_reply.started": "2025-02-18T13:08:28.839065Z"
        },
        "id": "7Sq30COO1Tq_",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "ensemble_probs = (resnet_probs + vgg_probs + swin_probs) / 3\n",
        "ensemble_preds = np.argmax(ensemble_probs, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T13:08:33.827032Z",
          "iopub.status.busy": "2025-02-18T13:08:33.826596Z",
          "iopub.status.idle": "2025-02-18T13:08:33.847174Z",
          "shell.execute_reply": "2025-02-18T13:08:33.846222Z",
          "shell.execute_reply.started": "2025-02-18T13:08:33.826996Z"
        },
        "id": "_OzqUBND1db5",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "individual_preds = np.stack([\n",
        "    np.argmax(resnet_probs, axis=1),\n",
        "    np.argmax(vgg_probs, axis=1),\n",
        "    np.argmax(swin_probs, axis=1)\n",
        "])\n",
        "\n",
        "# Perform majority voting\n",
        "from scipy.stats import mode\n",
        "ensemble_preds = mode(individual_preds, axis=0)[0].flatten()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-02-18T13:08:36.992604Z",
          "iopub.status.busy": "2025-02-18T13:08:36.992303Z",
          "iopub.status.idle": "2025-02-18T13:08:37.191836Z",
          "shell.execute_reply": "2025-02-18T13:08:37.190944Z",
          "shell.execute_reply.started": "2025-02-18T13:08:36.992582Z"
        },
        "id": "Q7wCGiIx1hp2",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "true_labels = np.argmax(test_labels, axis=1)\n",
        "\n",
        "print(f\"Ensemble Test Accuracy: {accuracy_score(true_labels, ensemble_preds) * 100:.2f}%\")\n",
        "print(\"\\nClassification Report:\\n\", classification_report(true_labels, ensemble_preds))\n",
        "\n",
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(true_labels, ensemble_preds)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"No\", \"Yes\"], yticklabels=[\"No\", \"Yes\"])\n",
        "plt.title(\"Confusion Matrix (Ensemble Model)\")\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x18p-BCktq94"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "# Assuming 'ensemble_probs' contains probabilities for the positive class (class 1)\n",
        "fpr, tpr, thresholds = roc_curve(true_labels, ensemble_probs[:, 1])  # Use probabilities for class 1\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic (ROC) Curve (Ensemble Model)')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cENdLvSiNb0V"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Assuming history is the training history object returned by model.fit\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Plot training & validation accuracy values\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('Model accuracy (Ensemble Model)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "\n",
        "# Plot training & validation loss values\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss (Ensemble Model)')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Validation'], loc='upper left')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7iXrdTJjuPhT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input\n",
        "from tensorflow.keras.models import load_model\n",
        "import torch\n",
        "import timm\n",
        "from collections import OrderedDict\n",
        "from scipy.stats import mode\n",
        "\n",
        "# Load the trained models\n",
        "resnet_model = load_model(\"/content/resnet50_model.h5\")\n",
        "vgg_model = load_model(\"/content/vgg16_model.h5\")\n",
        "\n",
        "# Instantiate SwinTransformer using timm.create_model\n",
        "swin_model = timm.create_model(\"swin_tiny_patch4_window7_224\", pretrained=False, num_classes=2)\n",
        "\n",
        "# Load the state_dict, handling the key prefix\n",
        "state_dict = torch.load(\"best_swin_model.pth\", map_location=torch.device(\"cpu\"))\n",
        "# Remove the 'swin.' prefix from the keys\n",
        "new_state_dict = OrderedDict()\n",
        "for k, v in state_dict.items():\n",
        "    name = k[5:]  # remove `swin.`\n",
        "    new_state_dict[name] = v\n",
        "# load params\n",
        "swin_model.load_state_dict(new_state_dict)\n",
        "swin_model.eval()\n",
        "\n",
        "\n",
        "def predict_glaucoma(image_path):\n",
        "    # Preprocess the image for all models\n",
        "    img = load_img(image_path, target_size=(224, 224))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    img_array_tf = preprocess_input(img_array)\n",
        "    img_array_torch = torch.tensor(img_array.transpose(0, 3, 1, 2), dtype=torch.float32)\n",
        "\n",
        "\n",
        "    # Make predictions\n",
        "    resnet_pred = resnet_model.predict(img_array_tf)\n",
        "    vgg_pred = vgg_model.predict(img_array_tf)\n",
        "    with torch.no_grad():\n",
        "        swin_pred = swin_model(img_array_torch)\n",
        "        swin_pred = torch.nn.functional.softmax(swin_pred, dim=1).numpy()\n",
        "\n",
        "    # Ensemble predictions\n",
        "    ensemble_prob = (resnet_pred + vgg_pred + swin_pred) / 3\n",
        "    ensemble_pred = np.argmax(ensemble_prob, axis=1)\n",
        "\n",
        "    # Return prediction\n",
        "    return ensemble_pred[0]  # Return the class (0 or 1)\n",
        "\n",
        "# Example usage\n",
        "image_path = \"/content/organized_data/aug_yes/images/037_2.jpg\"\n",
        "prediction = predict_glaucoma(image_path)\n",
        "\n",
        "if prediction == 1:\n",
        "  print(\"Glaucoma detected\")\n",
        "else:\n",
        "  print(\"No Glaucoma\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GvyMDofjutUG"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "image_path = \"/content/organized_data/no/images/007.jpg\"\n",
        "prediction = predict_glaucoma(image_path)\n",
        "\n",
        "if prediction == 1:\n",
        "  print(\"Glaucoma detected\")\n",
        "else:\n",
        "  print(\"No Glaucoma\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 6691388,
          "sourceId": 10783312,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30887,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}